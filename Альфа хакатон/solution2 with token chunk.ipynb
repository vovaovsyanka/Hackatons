{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b49838c8",
   "metadata": {},
   "source": [
    "Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e0d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fead1c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется устройство: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Используется устройство: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686da281",
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_df = pd.read_csv('websites_updated.csv')\n",
    "questions_df = pd.read_csv('questions_clean.csv')\n",
    "websites_df.set_index('web_id', inplace=True)\n",
    "questions_df.set_index('q_id', inplace=True)\n",
    "websites_df = websites_df.drop(1938)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05af2db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>kind</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>web_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>https://alfabank.ru/get-money/land/credit-holi...</td>\n",
       "      <td>html</td>\n",
       "      <td>Кредитные каникулы — Альфа-Банк</td>\n",
       "      <td>Кредитные каникулы — это возможность временно ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>https://alfabank.ru/help/t/retail/alfaforbusin...</td>\n",
       "      <td>html</td>\n",
       "      <td>Как вернуть деньги покупателю и как рассчитыва...</td>\n",
       "      <td>Возврат денег покупателю можно оформить через ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>https://alfabank.ru/help/articles/investments/...</td>\n",
       "      <td>html</td>\n",
       "      <td>Как вывести деньги с брокерского счёта — Альфа...</td>\n",
       "      <td>Вывести деньги с брокерского счёта можно на ка...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>https://alfabank.ru/make-money/investments/hel...</td>\n",
       "      <td>html</td>\n",
       "      <td>Пополнение и вывод средств — Альфа-Инвестиции</td>\n",
       "      <td>Вывести деньги с брокерского счёта можно на сл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>https://alfabank.ru/everyday/smart/</td>\n",
       "      <td>html</td>\n",
       "      <td>Альфа-Смарт — подписка Альфа-Банка</td>\n",
       "      <td>Альфа-Смарт — семейная подписка, запущенная в ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url  kind  \\\n",
       "web_id                                                            \n",
       "1933    https://alfabank.ru/get-money/land/credit-holi...  html   \n",
       "1934    https://alfabank.ru/help/t/retail/alfaforbusin...  html   \n",
       "1935    https://alfabank.ru/help/articles/investments/...  html   \n",
       "1936    https://alfabank.ru/make-money/investments/hel...  html   \n",
       "1937                  https://alfabank.ru/everyday/smart/  html   \n",
       "\n",
       "                                                    title  \\\n",
       "web_id                                                      \n",
       "1933                      Кредитные каникулы — Альфа-Банк   \n",
       "1934    Как вернуть деньги покупателю и как рассчитыва...   \n",
       "1935    Как вывести деньги с брокерского счёта — Альфа...   \n",
       "1936        Пополнение и вывод средств — Альфа-Инвестиции   \n",
       "1937                   Альфа-Смарт — подписка Альфа-Банка   \n",
       "\n",
       "                                                     text  \n",
       "web_id                                                     \n",
       "1933    Кредитные каникулы — это возможность временно ...  \n",
       "1934    Возврат денег покупателю можно оформить через ...  \n",
       "1935    Вывести деньги с брокерского счёта можно на ка...  \n",
       "1936    Вывести деньги с брокерского счёта можно на сл...  \n",
       "1937    Альфа-Смарт — семейная подписка, запущенная в ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "websites_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a708bf7",
   "metadata": {},
   "source": [
    "Предобработка и токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3804368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12e96edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\AdminWin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AdminWin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b4af3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('russian'))\n",
    "morph = pymorphy3.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67573135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1937/1937 [01:24<00:00, 22.82it/s] \n"
     ]
    }
   ],
   "source": [
    "processed_texts = []\n",
    "for idx, row in tqdm(websites_df.iterrows(), total=len(websites_df)):\n",
    "    full_text = f\"{row['title']} {row['text']}\"\n",
    "    text = full_text.lower()\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    tokens = word_tokenize(text, language='russian')\n",
    "    \n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if (token not in stop_words and \n",
    "            len(token) > 2 and \n",
    "            token.isalpha()):\n",
    "            lemma = morph.parse(token)[0].normal_form\n",
    "            processed_tokens.append(lemma)\n",
    "    \n",
    "    processed_text = ' '.join(processed_tokens)\n",
    "    processed_texts.append(processed_text)\n",
    "\n",
    "websites_df['processed_text'] = processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23dc1300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>kind</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>web_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>https://alfabank.ru/get-money/land/credit-holi...</td>\n",
       "      <td>html</td>\n",
       "      <td>Кредитные каникулы — Альфа-Банк</td>\n",
       "      <td>Кредитные каникулы — это возможность временно ...</td>\n",
       "      <td>кредитный каникулы альфа банк кредитный канику...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>https://alfabank.ru/help/t/retail/alfaforbusin...</td>\n",
       "      <td>html</td>\n",
       "      <td>Как вернуть деньги покупателю и как рассчитыва...</td>\n",
       "      <td>Возврат денег покупателю можно оформить через ...</td>\n",
       "      <td>вернуть деньга покупатель рассчитываться комис...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>https://alfabank.ru/help/articles/investments/...</td>\n",
       "      <td>html</td>\n",
       "      <td>Как вывести деньги с брокерского счёта — Альфа...</td>\n",
       "      <td>Вывести деньги с брокерского счёта можно на ка...</td>\n",
       "      <td>вывести деньга брокерский счёт альфа банк выве...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>https://alfabank.ru/make-money/investments/hel...</td>\n",
       "      <td>html</td>\n",
       "      <td>Пополнение и вывод средств — Альфа-Инвестиции</td>\n",
       "      <td>Вывести деньги с брокерского счёта можно на сл...</td>\n",
       "      <td>пополнение вывод средство альфа инвестиция выв...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>https://alfabank.ru/everyday/smart/</td>\n",
       "      <td>html</td>\n",
       "      <td>Альфа-Смарт — подписка Альфа-Банка</td>\n",
       "      <td>Альфа-Смарт — семейная подписка, запущенная в ...</td>\n",
       "      <td>альфа смарт подписка альфа банк альфа смарт се...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url  kind  \\\n",
       "web_id                                                            \n",
       "1933    https://alfabank.ru/get-money/land/credit-holi...  html   \n",
       "1934    https://alfabank.ru/help/t/retail/alfaforbusin...  html   \n",
       "1935    https://alfabank.ru/help/articles/investments/...  html   \n",
       "1936    https://alfabank.ru/make-money/investments/hel...  html   \n",
       "1937                  https://alfabank.ru/everyday/smart/  html   \n",
       "\n",
       "                                                    title  \\\n",
       "web_id                                                      \n",
       "1933                      Кредитные каникулы — Альфа-Банк   \n",
       "1934    Как вернуть деньги покупателю и как рассчитыва...   \n",
       "1935    Как вывести деньги с брокерского счёта — Альфа...   \n",
       "1936        Пополнение и вывод средств — Альфа-Инвестиции   \n",
       "1937                   Альфа-Смарт — подписка Альфа-Банка   \n",
       "\n",
       "                                                     text  \\\n",
       "web_id                                                      \n",
       "1933    Кредитные каникулы — это возможность временно ...   \n",
       "1934    Возврат денег покупателю можно оформить через ...   \n",
       "1935    Вывести деньги с брокерского счёта можно на ка...   \n",
       "1936    Вывести деньги с брокерского счёта можно на сл...   \n",
       "1937    Альфа-Смарт — семейная подписка, запущенная в ...   \n",
       "\n",
       "                                           processed_text  \n",
       "web_id                                                     \n",
       "1933    кредитный каникулы альфа банк кредитный канику...  \n",
       "1934    вернуть деньга покупатель рассчитываться комис...  \n",
       "1935    вывести деньга брокерский счёт альфа банк выве...  \n",
       "1936    пополнение вывод средство альфа инвестиция выв...  \n",
       "1937    альфа смарт подписка альфа банк альфа смарт се...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "websites_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d1c0e8",
   "metadata": {},
   "source": [
    "Чанкирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b376d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "import hashlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf8124c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=256,           # Количество токенов в чанке\n",
    "    chunk_overlap=64,         # Перекрытие между чанками\n",
    "    encoding_name=\"cl100k_base\"  # Кодировка для подсчета токенов (используется в GPT)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57c1e8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1937/1937 [00:01<00:00, 1005.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создано 22006 чанков из 1937 документов\n",
      "Средняя длина чанка: 597.5 символов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_chunks = []\n",
    "chunk_metadata = []\n",
    "\n",
    "for idx, row in tqdm(websites_df.iterrows(), total=len(websites_df)):\n",
    "    if len(row['processed_text'].strip()) < 50:\n",
    "        continue\n",
    "    \n",
    "    chunks = token_splitter.split_text(row['processed_text'])\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        if len(chunk.strip()) > 30:\n",
    "            chunk_id = hashlib.md5(f\"{idx}_{chunk_idx}\".encode()).hexdigest()[:8]\n",
    "            \n",
    "            all_chunks.append(chunk)\n",
    "            chunk_metadata.append({\n",
    "                'chunk_id': chunk_id,\n",
    "                'web_id': idx,\n",
    "                'chunk_index': chunk_idx,\n",
    "                'original_url': row.get('url', ''),\n",
    "                'text_length': len(chunk),\n",
    "            })\n",
    "\n",
    "print(f\"Создано {len(all_chunks)} чанков из {len(websites_df)} документов\")\n",
    "print(f\"Средняя длина чанка: {np.mean([m['text_length'] for m in chunk_metadata]):.1f} символов\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653c966c",
   "metadata": {},
   "source": [
    "Векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07803993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0399b373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AdminWin\\AppData\\Local\\Temp\\ipykernel_16832\\3961618539.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "311ebc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i, chunk_text in enumerate(all_chunks):\n",
    "    doc = Document(\n",
    "        page_content=chunk_text,\n",
    "        metadata=chunk_metadata[i]\n",
    "    )\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26abe37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de779757",
   "metadata": {},
   "source": [
    "Векторный поиск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42141af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14a8d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_for_bm25 = [doc.page_content for doc in documents]\n",
    "tokenized_corpus = [text.split() for text in texts_for_bm25]\n",
    "bm25_index = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6755c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, vector_store, bm25_index, documents, top_k=5, alpha=0.7):\n",
    "    \"\"\"Гибридный поиск: комбинация семантического (FAISS) и ключевого (BM25)\"\"\"\n",
    "    processed_query = query.lower()\n",
    "    processed_query = re.sub(r'<[^>]+>', '', processed_query)\n",
    "    processed_query = re.sub(r'[^\\w\\s]', ' ', processed_query)\n",
    "    processed_query = re.sub(r'\\s+', ' ', processed_query).strip()\n",
    "    \n",
    "    tokens = word_tokenize(processed_query, language='russian')\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if (token not in stop_words and len(token) > 2 and token.isalpha()):\n",
    "            lemma = morph.parse(token)[0].normal_form\n",
    "            processed_tokens.append(lemma)\n",
    "    \n",
    "    processed_query = ' '.join(processed_tokens)\n",
    "    \n",
    "    semantic_results = vector_store.similarity_search_with_score(\n",
    "        processed_query, \n",
    "        k=top_k * 3\n",
    "    )\n",
    "    \n",
    "    semantic_scores = {}\n",
    "    for doc, score in semantic_results:\n",
    "        chunk_id = doc.metadata['chunk_id']\n",
    "        similarity = 1 - score\n",
    "        if chunk_id not in semantic_scores:\n",
    "            semantic_scores[chunk_id] = similarity\n",
    "    \n",
    "    # Ключевой поиск через BM25\n",
    "    tokenized_query = processed_query.split()\n",
    "    if tokenized_query:\n",
    "        bm25_scores = bm25_index.get_scores(tokenized_query)\n",
    "        \n",
    "        keyword_scores = {}\n",
    "        for idx, score in enumerate(bm25_scores):\n",
    "            chunk_id = documents[idx].metadata['chunk_id']\n",
    "            if chunk_id not in keyword_scores:\n",
    "                keyword_scores[chunk_id] = score\n",
    "    else:\n",
    "        keyword_scores = {}\n",
    "    \n",
    "    # Нормализация scores\n",
    "    if semantic_scores:\n",
    "        max_semantic = max(semantic_scores.values())\n",
    "        for chunk_id in semantic_scores:\n",
    "            semantic_scores[chunk_id] /= max_semantic if max_semantic > 0 else 1\n",
    "\n",
    "    if keyword_scores:\n",
    "        max_keyword = max(keyword_scores.values())\n",
    "        for chunk_id in keyword_scores:\n",
    "            keyword_scores[chunk_id] /= max_keyword if max_keyword > 0 else 1\n",
    "\n",
    "    # Комбинирование scores\n",
    "    combined_scores = {}\n",
    "    all_chunk_ids = set(list(semantic_scores.keys()) + list(keyword_scores.keys()))\n",
    "    \n",
    "    for chunk_id in all_chunk_ids:\n",
    "        semantic_score = semantic_scores.get(chunk_id, 0)\n",
    "        keyword_score = keyword_scores.get(chunk_id, 0)\n",
    "        combined_score = alpha * semantic_score + (1 - alpha) * keyword_score\n",
    "        combined_scores[chunk_id] = combined_score\n",
    "    \n",
    "    # Возвращаем топ-K документов\n",
    "    top_chunk_ids = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k*3]\n",
    "    return [chunk_id for chunk_id, score in top_chunk_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec81716",
   "metadata": {},
   "source": [
    "Re-Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0cc0b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a6e1802",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db876b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_reranking(query, vector_store, bm25_index, documents, top_k=5):\n",
    "    \"\"\"Поиск с реранжированием\"\"\"\n",
    "    candidate_chunk_ids = hybrid_search(query, vector_store, bm25_index, documents, top_k=top_k * 3)\n",
    "    \n",
    "    candidate_texts = []\n",
    "    chunk_id_to_web_id = {}  # Маппинг chunk_id → web_id\n",
    "    \n",
    "    for chunk_id in candidate_chunk_ids:\n",
    "        doc = next((d for d in documents if d.metadata.get('chunk_id') == chunk_id), None)\n",
    "        candidate_texts.append(doc.page_content)\n",
    "        chunk_id_to_web_id[chunk_id] = doc.metadata['web_id']\n",
    "    \n",
    "    # Оценка релевантности пар (запрос, документ)\n",
    "    pairs = [[query, doc_text] for doc_text in candidate_texts]\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    # Сортировка по убыванию релевантности\n",
    "    scored_candidates = []\n",
    "    for i, (chunk_id, score) in enumerate(zip(candidate_chunk_ids, scores)):\n",
    "        web_id = chunk_id_to_web_id.get(chunk_id)\n",
    "        scored_candidates.append((web_id, score, chunk_id))\n",
    "    \n",
    "    # Сортировка по убыванию релевантности\n",
    "    scored_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Убираем дубликаты web_id, оставляя только лучший результат для каждого web_id\n",
    "    unique_web_ids = []\n",
    "    seen_web_ids = set()\n",
    "    \n",
    "    for web_id, score, chunk_id in scored_candidates:\n",
    "        if web_id not in seen_web_ids:\n",
    "            unique_web_ids.append(web_id)\n",
    "            seen_web_ids.add(web_id)\n",
    "        if len(unique_web_ids) >= top_k:\n",
    "            break\n",
    "    \n",
    "    return unique_web_ids[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1139602",
   "metadata": {},
   "source": [
    "Выполнение поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ff0d1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6977/6977 [48:02<00:00,  2.42it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>web_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1557, 900, 835, 372, 1704]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1557, 1239, 108, 415, 1798]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[1704, 1758, 1760, 344, 1705]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[1038, 1032, 343, 1042, 1760]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[199, 175, 1039, 1583, 164]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   q_id                       web_list\n",
       "0     1    [1557, 900, 835, 372, 1704]\n",
       "1     2   [1557, 1239, 108, 415, 1798]\n",
       "2     3  [1704, 1758, 1760, 344, 1705]\n",
       "3     4  [1038, 1032, 343, 1042, 1760]\n",
       "4     5    [199, 175, 1039, 1583, 164]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for q_id, row in tqdm(questions_df.iterrows(), total=len(questions_df)):\n",
    "    query = row['query']\n",
    "    \n",
    "    top_web_ids = search_with_reranking(\n",
    "        query, vector_store, bm25_index, documents, top_k=5\n",
    "    )\n",
    "    \n",
    "    web_list_str = f\"[{', '.join(map(str, top_web_ids))}]\"\n",
    "    results.append({'q_id': q_id, 'web_list': web_list_str})\n",
    "\n",
    "# Сохранение результатов\n",
    "submission_df = pd.DataFrame(results)\n",
    "with open('submit.csv', 'w', encoding='utf-8') as f:\n",
    "    f.write('q_id,web_list\\n')\n",
    "    for result in results:\n",
    "        f.write(f'{result[\"q_id\"]},\"{result[\"web_list\"]}\"\\n')\n",
    "\n",
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
